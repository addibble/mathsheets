\documentclass{article}
\usepackage[utf8]{inputenc} % Handle UTF-8 encoding
\usepackage{amsmath, amssymb, tikz, geometry, multicol}
\usetikzlibrary{calc}
\geometry{margin=0.15in}
\tolerance=1000

\begin{document}

% Start of the document with two columns
\begin{multicols}{2}

\section*{Vector Notation and Representation}

\subsection*{Unit Vector Notation}

Vectors can be represented in different notations:

- \textbf{Angle Bracket Notation}:

  A vector $\vec{V}$ in two dimensions can be written as:

  
  \[
  \vec{V} = \langle V_x, V_y \rangle
  \]

- \textbf{Column Vector Notation}:

  \[
  \vec{V} = \begin{bmatrix} V_x \\ V_y \end{bmatrix}
  \]

- \textbf{Cartesian Coordinates}:

  Using the unit vectors $\hat{i}$ and $\hat{j}$:

  \[
  \vec{V} = V_x \hat{i} + V_y \hat{j}
  \]

Where:

- $ V_x $ is the component of the vector along the $ x $-axis.
- $ V_y $ is the component of the vector along the $ y $-axis.
- $ \hat{i} $ and $ \hat{j} $ are unit vectors in the $ x $ and $ y $ directions, respectively.

\subsection*{Zero-Length and Parallel Vectors}

- \textbf{Zero-Length Vector (Zero Vector)}:

  A vector with zero magnitude and no specific direction.

  \[
  \vec{0} = \langle 0, 0 \rangle
  \]

  \textbf{Properties:}

  - Adding the zero vector to any vector leaves the original vector unchanged.

    \[
    \vec{V} + \vec{0} = \vec{V}
    \]

- \textbf{Parallel Vectors}:

  Two vectors are parallel if they have the same or opposite directions.

  \textbf{Conditions for Parallelism:}

  - Vectors $\vec{A}$ and $\vec{B}$ are parallel if:

    \[
    \vec{A} = k \vec{B}
    \]

    where $ k $ is a scalar constant.

    - If $ k > 0 $, vectors are in the same direction.
    - If $ k < 0 $, vectors are in opposite directions.

  - \textbf{Component Proportionality:}

    \[
    \frac{A_x}{B_x} = \frac{A_y}{B_y}
    \]

    If the ratios of the corresponding components are equal (and finite), the vectors are parallel.

\subsection*{Position Vectors and Vector Difference}

If $ A $ and $ B $ are points in the Cartesian plane with position vectors $ \vec{a} $ and $ \vec{b} $ respectively, then the vector from point $ A $ to point $ B $ is given by:

\[
\vec{AB} = \vec{b} - \vec{a}
\]

This represents the displacement from point $ A $ to point $ B $.

\textbf{Example:}

Let point $ A $ have coordinates $ (2, 3) $ and point $ B $ have coordinates $ (5, 7) $.

- Write the position vectors:

  \[
  \vec{a} = \begin{bmatrix} 2 \\ 3 \end{bmatrix}, \quad \vec{b} = \begin{bmatrix} 5 \\ 7 \end{bmatrix}
  \]

- Compute $ \vec{AB} $:

  \[
  \vec{AB} = \vec{b} - \vec{a} = \begin{bmatrix} 5 \\ 7 \end{bmatrix} - \begin{bmatrix} 2 \\ 3 \end{bmatrix} = \begin{bmatrix} 5 - 2 \\ 7 - 3 \end{bmatrix} = \begin{bmatrix} 3 \\ 4 \end{bmatrix}
  \]

- The magnitude of $ \vec{AB} $ is:

  \[
  |\vec{AB}| = \sqrt{3^2 + 4^2} = 5
  \]

- The direction (angle) of $ \vec{AB} $ with respect to the horizontal axis is:

  \[
  \theta = \arctan\left( \frac{4}{3} \right) \approx 53.13^\circ
  \]

\section*{Vector Decomposition and Addition}

\subsection*{Decomposing Vectors into Components}

Any vector $ \vec{V} $ can be decomposed into its horizontal and vertical components using trigonometric functions.

Given a vector of magnitude $ V $ at an angle $ \theta $ with respect to the horizontal axis:

\[
\begin{aligned}
V_x &= V \cos \theta \\
V_y &= V \sin \theta
\end{aligned}
\]

\subsection*{Adding Vectors Using Components}

To add two vectors, decompose each vector into its components, add the corresponding components, and then recombine the resultant components to find the resultant vector.

Given two vectors $ \vec{A} $ and $ \vec{B} $:

\[
\begin{aligned}
A_x &= A \cos \alpha \\
A_y &= A \sin \alpha \\
B_x &= B \cos \beta \\
B_y &= B \sin \beta
\end{aligned}
\]

The resultant vector $ \vec{R} = \vec{A} + \vec{B} $ has components:

\[
\begin{aligned}
R_x &= A_x + B_x \\
R_y &= A_y + B_y
\end{aligned}
\]

The magnitude and direction of $ \vec{R} $ are:

\[
\begin{aligned}
R &= \sqrt{R_x^2 + R_y^2} \\
\theta_R &= \arctan\left( \frac{R_y}{R_x} \right)
\end{aligned}
\]

\subsection*{Dot Product and Cosine Similarity}

The \textbf{dot product} (or scalar product) of two vectors $\vec{A} = \langle A_x, A_y, A_z \rangle$ and $\vec{B} = \langle B_x, B_y, B_z \rangle$ is defined as:

\[
\vec{A} \cdot \vec{B} = A_x B_x + A_y B_y + A_z B_z
\]

The magnitudes of the vectors are $\|\vec{A}\| = \sqrt{A_x^2 + A_y^2 + A_z^2}$ and $\|\vec{B}\| = \sqrt{B_x^2 + B_y^2 + B_z^2}$.  

The \textbf{cosine similarity}, which gives the cosine of the angle $\theta$ between $\vec{A}$ and $\vec{B}$, is:

\[
\cos \theta = \frac{\vec{A} \cdot \vec{B}}{\|\vec{A}\| \\, \|\vec{B}\|}
\]

\subsection*{Perpendicularity of Vectors}

Two vectors $\vec{A}$ and $\vec{B}$ are \textbf{perpendicular} (orthogonal) if and only if their dot product is zero:

\[
\vec{A} \cdot \vec{B} = 0 \quad \Longleftrightarrow \quad \cos \theta = 0 \quad (\theta = 90^\circ)
\]

This condition can be checked algebraically using the component form of the dot product or geometrically using the cosine of the angle.

\subsection*{Vector Equation of a Plane}

A plane in three dimensions can be defined by a point $P_0(x_0,y_0,z_0)$ on the plane and a normal vector $\vec{n} = \langle a,b,c \rangle$ perpendicular to the plane.  

Let $\vec{r} = \langle x,y,z \rangle$ be the position vector of an arbitrary point $P(x,y,z)$ on the plane, and let $\vec{r}_0 = \langle x_0,y_0,z_0 \rangle$ be the position vector of $P_0$.  

The \textbf{vector equation of the plane} is:

\[
\vec{n} \cdot (\vec{r} - \vec{r}_0) = 0
\]

Equivalently, the \textbf{scalar equation} of the plane is:

\[
a(x - x_0) + b(y - y_0) + c(z - z_0) = 0
\]

\section*{Matrices}

\subsection*{Matrix Multiplication Algorithm}
To multiply two matrices $ A $ and $ B $:

- Let $ A $ be an $ m \times n $ matrix and $ B $ an $ n \times p $ matrix.
- The product $ C = AB $ is an $ m \times p $ matrix where:

\[
C_{ij} = \sum_{k=1}^n A_{ik} B_{kj}
\]

\textbf{Example:} Let
\[
A = \begin{bmatrix}1 & 2 & 3\\4 & 5 & 6\end{bmatrix},
\quad
B = \begin{bmatrix}7 & 8\\9 & 10\\11 & 12\end{bmatrix}.
\]
Then
\[
AB = \begin{bmatrix}
1\cdot7 + 2\cdot9 + 3\cdot11 & 1\cdot8 + 2\cdot10 + 3\cdot12\\
4\cdot7 + 5\cdot9 + 6\cdot11 & 4\cdot8 + 5\cdot10 + 6\cdot12
\end{bmatrix}
= \begin{bmatrix}58 & 64\\139 & 154\end{bmatrix}.
\]

\subsection*{Matrices and Vectors}
- A vector $\vec{v} \in \mathbb{R}^n$ can be represented as a column matrix:
\[
\vec{v} = \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix}.
\]
- To transform $\vec{v}$ using a matrix $A$:
\[
\vec{v}' = A\vec{v}.
\]

\textbf{Example:} Let
\[
A = \begin{bmatrix}2 & 0\\-1 & 3\end{bmatrix},
\quad
\vec{v} = \begin{bmatrix}x\\y\end{bmatrix}.
\]
Then
\[
A\vec{v} = \begin{bmatrix}2x + 0\cdot y\\-1\cdot x + 3y\end{bmatrix}.
\]

\subsection*{Determinant of a Matrix}
For a square matrix $A$:

- \textbf{$2\times2$:}
\[
\det\!\begin{bmatrix}a & b\\c & d\end{bmatrix} = ad - bc.
\]

\textbf{Example:} For $A=\begin{bmatrix}2 & 3\\1 & 4\end{bmatrix}$,
$\det(A)=2\cdot4 - 3\cdot1 = 5$.

- \textbf{$3\times3$:}
\[
\det\!\begin{bmatrix}a & b & c\\d & e & f\\g & h & i\end{bmatrix}
= a(ei - fh) - b(di - fg) + c(dh - eg).
\]

\textbf{Example:} For
$A=\begin{bmatrix}1 & 2 & 3\\4 & 5 & 6\\7 & 8 & 9\end{bmatrix}$,
$\det(A)=0$ (rows are linearly dependent).

\subsection*{Inverse of a $2\times2$ Matrix Using the Determinant}
Given a $2\times2$ matrix
\[
A = \begin{bmatrix}a & b\\c & d\end{bmatrix},
\]
if $\det(A) = ad - bc \neq 0$, then the inverse of $A$ is given by:
\[
A^{-1} = \frac{1}{\det(A)}\begin{bmatrix}d & -b\\-c & a\end{bmatrix}.
\]

\textbf{Example:} Let $A=\begin{bmatrix}2 & 3\\1 & 4\end{bmatrix}$. We already found that $\det(A)=5$, so:
\[
A^{-1} = \frac{1}{5}\begin{bmatrix}4 & -3\\-1 & 2\end{bmatrix}
= \begin{bmatrix}0.8 & -0.6\\-0.2 & 0.4\end{bmatrix}.
\]

\subsection*{Matrix Minor and Cofactor}
- The \emph{minor} $M_{ij}$ of element $A_{ij}$ is the determinant of the matrix formed by deleting the $i$-th row and $j$-th column.
- The \emph{cofactor} is $C_{ij}=(-1)^{i+j}M_{ij}$.

\textbf{Example:} Let
\[
A = \begin{bmatrix}3 & 0 & 2\\2 & 0 & -2\\0 & 1 & 1\end{bmatrix}.
\]
Then
\[
M_{1,3} = \det\!\begin{bmatrix}2 & 0\\0 & 1\end{bmatrix} = 2,
\quad
C_{1,3} = (-1)^{1+3}\cdot2 = 2.
\]

\subsection*{Cross Product via Cofactor Expansion}
Given two 3D vectors
\[
\mathbf{a} = \langle a_1, a_2, a_3 \rangle, \quad
\mathbf{b} = \langle b_1, b_2, b_3 \rangle,
\]
the cross product $\mathbf{a} \times \mathbf{b}$ is defined as the determinant of the following $3\times3$ matrix:
\[
\mathbf{a} \times \mathbf{b} =
\begin{vmatrix}
\mathbf{i} & \mathbf{j} & \mathbf{k} \\
a_1 & a_2 & a_3 \\
b_1 & b_2 & b_3
\end{vmatrix}
= \mathbf{i}
\begin{vmatrix}
a_2 & a_3\\
b_2 & b_3
\end{vmatrix}
- \mathbf{j}
\begin{vmatrix}
a_1 & a_3\\
b_1 & b_3
\end{vmatrix}
+ \mathbf{k}
\begin{vmatrix}
a_1 & a_2\\
b_1 & b_2
\end{vmatrix}.
\]

\textbf{Example:} Let
\[
\mathbf{a} = \langle 2, -1, 5 \rangle, \quad
\mathbf{b} = \langle -1, 4, 2 \rangle.
\]
Then:
\begin{align*}
\mathbf{i}: & \quad
\begin{vmatrix}-1 & 5\\ 4 & 2\end{vmatrix} = (-1)(2) - (5)(4) = -2 - 20 = -22,\\
\mathbf{j}: & \quad
\begin{vmatrix}2 & 5\\ -1 & 2\end{vmatrix} = (2)(2) - (5)(-1) = 4 + 5 = 9,\\
\mathbf{k}: & \quad
\begin{vmatrix}2 & -1\\ -1 & 4\end{vmatrix} = (2)(4) - (-1)(-1) = 8 - 1 = 7.
\end{align*}

So,
\[
\mathbf{a} \times \mathbf{b} = -22\,\mathbf{i} - 9\,\mathbf{j} + 7\,\mathbf{k} = \langle -22, -9, 7 \rangle.
\]

\subsection*{Identity Matrix}
The identity matrix $I_n$ is the $n\times n$ matrix with ones on the diagonal and zeros elsewhere.
\[
I_3 = \begin{bmatrix}1 & 0 & 0\\0 & 1 & 0\\0 & 0 & 1\end{bmatrix},
\quad AI_n = I_nA = A.
\]

\subsection*{Augmented Matrices}
An augmented matrix $[A|\vec b]$ represents a system $A\vec x = \vec b$.

\textbf{Example:} The system
\[
\begin{cases}
x - y + 2z = 3 \\
2x + 3y + z = 7 \\
- x + 4y + 5z = -2
\end{cases}
\]
has augmented matrix
\[
\left[\begin{array}{ccc|c}
1 & -1 & 2 & 3 \\
2 & 3 & 1 & 7 \\
-1 & 4 & 5 & -2
\end{array}\right].
\]

\subsection*{Easy-to-Invert Matrices}
Certain matrix classes have straightforward inverses:

\paragraph{Diagonal Matrices}
A diagonal matrix $D=\operatorname{diag}(d_1,\dots,d_n)$ is invertible iff $d_i\neq0$ for all $i$, with
\[
D^{-1} = \operatorname{diag}(d_1^{-1},\dots,d_n^{-1}).
\]
\textbf{Example:} If
$D=\begin{bmatrix}2 & 0 & 0\\0 & 3 & 0\\0 & 0 & 4\end{bmatrix}$,
then
$D^{-1}=\begin{bmatrix}1/2 & 0 & 0\\0 & 1/3 & 0\\0 & 0 & 1/4\end{bmatrix}$.

\paragraph{Permutation Matrices}
A permutation matrix $P$ is obtained by permuting rows of $I_n$. It satisfies $P^{-1}=P^T$.
\textbf{Example:} Swapping rows 1 and 2 in $I_3$ gives
$P=\begin{bmatrix}0 & 1 & 0\\1 & 0 & 0\\0 & 0 & 1\end{bmatrix}$,
so $P^{-1}=P^T=P$.

\paragraph{Frobenius (Shear) Matrices}
A Frobenius (or elementary shear) matrix is of the form
\[
F = I + U,
\]
where all nonzero off-diagonal entries of $U$ lie in a single column (or row) and $U^2=0$. Hence
\[
F^{-1} = I - U.
\]

\textbf{Example:} Let
\[
U = \begin{bmatrix}
0 & 0 & 0 & 0\\
-1 & 0 & 0 & 0\\
4 & 0 & 0 & 0\\
-2 & 0 & 0 & 0
\end{bmatrix},
\quad
F = I_4 + U = \begin{bmatrix}
1 & 0 & 0 & 0\\
-1 & 1 & 0 & 0\\
4 & 0 & 1 & 0\\
-2 & 0 & 0 & 1
\end{bmatrix}.
\]
Then
\[
F^{-1} = I_4 - U = \begin{bmatrix}
1 & 0 & 0 & 0\\
1 & 1 & 0 & 0\\
-4 & 0 & 1 & 0\\
2 & 0 & 0 & 1
\end{bmatrix}.
\]

\subsection*{Row Echelon Form and Gaussian Elimination}

\textbf{Row Echelon Form (REF)}:
A matrix is in REF if:
\begin{itemize}
  \item All nonzero rows are above rows of all zeros.
  \item The leading entry (pivot) of each nonzero row is to the right of the pivot above.
  \item All entries below a pivot are zero.
\end{itemize}

\textbf{Gaussian Elimination Algorithm}: To solve $A\vec{x}= \vec{b}$:
\begin{enumerate}
  \item Form $[A|\vec{b}]$.
  \item Apply row operations (swap, scale, add multiples) to reach REF.
  \item (Optional) Continue to RREF for immediate solutions.
  \item Back-substitute if in REF.
\end{enumerate}

\textbf{Example}: Solve
\[
\begin{cases}
2x + 3y = 5 \\
4x - y = 6
\end{cases}
\]
Augmented:
\[
\left[\begin{array}{cc|c}
2 & 3 & 5 \\
4 & -1 & 6
\end{array}\right].
\]
Steps lead to RREF and the solution $(x,y)=(19/7,4/7)$.

\subsection*{Reduced Row Echelon Form (RREF)}
In addition to REF:
\begin{itemize}
  \item Each pivot = 1.
  \item Each pivot is the only nonzero in its column.
\end{itemize}

\textbf{Example}: For
\[
A=\begin{bmatrix}1 & 2 & 1\\2 & 4 & 0\end{bmatrix},
\]
\[
\mathrm{RREF}(A)=\begin{bmatrix}1 & 2 & 1\\0 & 0 & -2\end{bmatrix}\xrightarrow{\div(-2)}\begin{bmatrix}1 & 2 & 1\\0 & 0 & 1\end{bmatrix}\xrightarrow{\text{textsubtract}}\begin{bmatrix}1 & 2 & 0\\0 & 0 & 1\end{bmatrix}.
\]

\end{multicols}

\end{document}

